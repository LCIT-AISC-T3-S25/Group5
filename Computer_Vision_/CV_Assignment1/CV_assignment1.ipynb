{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc2cb96-726e-4df9-8cc4-80176a2b7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc1eacb-5066-420b-8830-25e55af81576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === ‚úÖ BASE PATH ===\n",
    "BASE_PATH = \"C:/Users/moksh/OneDrive/Documents/OneDrive/Desktop/LCIT/CV/Assignment_1\"\n",
    "\n",
    "# === ‚úÖ LOAD DATA ===\n",
    "X_img_train = np.load(os.path.join(BASE_PATH, \"X_img_train_bal_q4.npy\"), allow_pickle=True)\n",
    "X_img_val = np.load(os.path.join(BASE_PATH, \"X_img_val_bal_q4.npy\"), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(BASE_PATH, \"y_train_bal_q4.npy\"))\n",
    "y_val = np.load(os.path.join(BASE_PATH, \"y_val_bal_q4.npy\"))\n",
    "\n",
    "with open(os.path.join(BASE_PATH, \"metadata_seq.pkl\"), \"rb\") as f:\n",
    "    X_meta_full = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(BASE_PATH, \"label_encoder.pkl\"), \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "X_meta_train = X_meta_full[:len(y_train)]\n",
    "X_meta_val = X_meta_full[len(y_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2183e57b-beaa-47da-94e5-b0f588eb342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: Counter({3: 1342, 0: 1342, 2: 1342, 4: 1342, 1: 1342})\n",
      "Val labels: Counter({4: 336, 1: 336, 0: 336, 2: 336, 3: 336})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Train labels:\", Counter(y_train))\n",
    "print(\"Val labels:\", Counter(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577d5ee4-f0ee-468f-997f-5cdf185d8ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape: (6710, 500)\n",
      "Metadata sample:\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Metadata variance: 0.014580092279379267\n"
     ]
    }
   ],
   "source": [
    "print(\"Metadata shape:\", X_meta_train.shape)\n",
    "print(\"Metadata sample:\\n\", X_meta_train[2])\n",
    "print(\"Metadata variance:\", np.var(X_meta_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad5bbe-415f-446f-8e59-ed31b96337b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "üîß Running config 1: {'dense_units': 128, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moksh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 4s/step - accuracy: 0.1869 - loss: 1.6622 - val_accuracy: 0.2000 - val_loss: 1.6099\n",
      "Epoch 2/5\n",
      "\u001b[1m 26/210\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m10:45\u001b[0m 4s/step - accuracy: 0.2213 - loss: 1.6106"
     ]
    }
   ],
   "source": [
    "# VGG16 Image-Only Training with Hyperparameter Tuning and Fine-Tuning\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# === BASE PATH ===\n",
    "BASE_PATH = r\"C:\\\\Users\\\\moksh\\\\OneDrive\\\\Documents\\\\OneDrive\\\\Desktop\\\\LCIT\\\\CV\\\\Assignment_1\"\n",
    "\n",
    "# === LOAD DATA ===\n",
    "X_img_train = np.load(os.path.join(BASE_PATH, \"X_img_train_bal_q4.npy\"), allow_pickle=True)\n",
    "X_img_val = np.load(os.path.join(BASE_PATH, \"X_img_val_bal_q4.npy\"), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(BASE_PATH, \"y_train_bal_q4.npy\"))\n",
    "y_val = np.load(os.path.join(BASE_PATH, \"y_val_bal_q4.npy\"))\n",
    "\n",
    "# === IMAGE PREPROCESSING ===\n",
    "def preprocess_image(path, target_size=(224, 224)):\n",
    "    image = cv2.imread(path)\n",
    "    if image is None:\n",
    "        return np.zeros((*target_size, 3))\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "    yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])\n",
    "    image = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "# === IMAGE-ONLY GENERATOR ===\n",
    "class ImageOnlyGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size=32, target_size=(224,224), augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idxs = slice(idx * self.batch_size, (idx + 1) * self.batch_size)\n",
    "        batch_imgs = [self.load_and_preprocess(p) for p in self.image_paths[idxs]]\n",
    "        batch_labels = self.labels[idxs]\n",
    "        return np.array(batch_imgs, dtype=np.float32), np.array(batch_labels, dtype=np.int32)\n",
    "\n",
    "    def load_and_preprocess(self, path):\n",
    "        image = preprocess_image(path, self.target_size)\n",
    "        if self.augment and np.random.rand() < 0.5:\n",
    "            image = np.fliplr(image)\n",
    "        return image\n",
    "\n",
    "# === MODEL BUILDER (IMAGE ONLY) ===\n",
    "def build_image_only_model(dense_units=256, dropout_rate=0.3, learning_rate=1e-4):\n",
    "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=image_input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# === GRID SEARCH ===\n",
    "param_grid = [\n",
    "    {'dense_units': 128, 'dropout_rate': 0.3, 'learning_rate': 1e-3},\n",
    "    {'dense_units': 256, 'dropout_rate': 0.3, 'learning_rate': 1e-4},\n",
    "    {'dense_units': 512, 'dropout_rate': 0.3, 'learning_rate': 5e-5}\n",
    "]\n",
    "\n",
    "train_gen = ImageOnlyGenerator(X_img_train, y_train, augment=True)\n",
    "val_gen = ImageOnlyGenerator(X_img_val, y_val, augment=False)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "best_model = None\n",
    "best_val_acc = 0\n",
    "best_config = None\n",
    "\n",
    "for i, params in enumerate(param_grid, 1):\n",
    "    K.clear_session()\n",
    "    print(f\"\\nüîß Running config {i}: {params}\")\n",
    "    model = build_image_only_model(**params)\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=[early_stop], verbose=1)\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model\n",
    "        best_config = params\n",
    "    print(f\"‚úÖ Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# === FINE-TUNING BEST MODEL ===\n",
    "print(f\"\\n Best config: {best_config}, Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "for layer in best_model.layers:\n",
    "    if hasattr(layer, \"name\") and layer.name in ['block5_conv1', 'block5_conv2', 'block5_conv3']:\n",
    "        layer.trainable = True\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "fine_tune_history = best_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2),\n",
    "        ModelCheckpoint(os.path.join(BASE_PATH, 'vgg_imgonly_finetuned.keras'), save_best_only=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\" Fine-Tuning Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97861c0c-8886-4d87-ad27-cf3032832daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
